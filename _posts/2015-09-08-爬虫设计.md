---
layout: post
title: 爬虫设计
keywords: 爬虫,spider, crawler
description: 爬虫设计
tags: [ spider, crawler ]
---

# 爬虫设计

## 功能分析

## 功能模块

### 爬虫

> URL管理和调度下载解析等

- Seeder

> 负责URL管理，也就是定向网站种子url，以及曾经抓过的所有url，可扩展存储网页指纹，历史抓取信息等

- Manager

> 负责调度，根据你的抓取策略(定时、增量、随机等)从Seeder调取url生成抓取任务；

- Harvester

> 有多个，竞争得到抓取任务并专心下载，下载后内容传递给Collector;

- Collector

> 有俩任务，解析html、搜集新链接，并把解析后的内容根据需要传递给索引系统和存储系统，把搜集的新连接传递给Seeder模块。

### 索引

> 用于全文检索

### 存储

> 解析后的内容和快照等

## 说明

1. Seeder+Manager 负责长期目标：指定的下载策略控制，根据不同的适用场景可以由不同的算法；
2. Harvester+Collector 负责短期目标：充分发挥计算机资源(CPU/Memory/IO/网络)完成下载和内容解析；

## 问题

- 根(root)

- 环路(cycle)

    - 别名环路
    - 文件系统链接环路

- 规范化URL

    - 端口
    - 转移符
    - \#标签
    - ip与域名对应,包括虚拟主机的设置
    - url的大小写

- 判定某个URL是否被访问过

    - 树和散列表
    - 有损的存在位图
    - 检查点
    - 分类(分布式)

## 相关算法

- 一致性Hash算法 : 并行任务分配

- 队列调度算法

- 布隆过滤器 (Bloom Filter) : 去重

    > 建立一个大的位数组，然后用多个Hash函数对同一个url进行hash得到多个数字，然后将位数组中这些数字对应的位置为1。下次再来一个url时，同样是用多个Hash函数进行hash，得到多个数字，我们只需要判断位数组中这些数字对应的为是全为1，如果全为1，那么说明这个url已经出现过

- 广度优先


## 相关资料

1. [How to write a crawler][link1]
2. Managing Gigabytes : Compressing and Indexing Documents and Images (<<海量数据管理 - 文档和图像的压缩与索引>>), Morgan Kaufmann出版社出版


[link1]:http://www.emanueleminotto.it/how-to-write-a-crawler

待续...